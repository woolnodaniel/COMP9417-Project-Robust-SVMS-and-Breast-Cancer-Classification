\documentclass[11pt]{article}

\usepackage{amssymb, amsmath, amsthm}
\usepackage[arrow,matrix,curve,cmtip,ps]{xy}
\usepackage{bm}
\usepackage{verbatim}
\usepackage[toc,page]{appendix}
\usepackage[english]{babel}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{geometry}
\geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=30mm,
 bottom=20mm,
}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage[section]{placeins}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage[hyphens]{url}

\setlength{\parindent}{0.0em}
\setlength{\parskip}{0.5em}
\renewcommand{\baselinestretch}{1.3}

\newcommand\Tstrut{\rule{0pt}{2.6ex}}
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}
\newcommand{\tline}{\Tstrut\Bstrut\\}

\pagestyle{fancy}
\fancyhf{}
\rhead{Daniel Woolnough z5116128}
\chead{COMP9417 - Project}
\lhead{\today}
\cfoot{\thepage}

\newcommand{\vect}[1]{\mbox{$\bm{#1}$}}
\newcommand{\ds}{\displaystyle}

%\addtolength{\topmargin}{-3\baselineskip}
%\addtolength{\textheight}{6\baselineskip}
%\addtolength{\textwidth}{2cm}
%\addtolength{\oddsidemargin}{-1cm}
%\addtolength{\evensidemargin}{-1cm}

\begin{document}

\begin{center}
\textbf{\large{COMP9417 Machine Learning Project}}
\\[10pt]
\textbf{\huge{Classification via Robust SVMs: A Comparison on the Wisconsin Breast Cancer Dataset}}
\\[10pt]
{\it \large{Daniel Woolnough, z5116128, Group h5116128, \today}}
\end{center}
\section*{Introduction}
Support Vector Machines (SVMs) are a useful machine learning tool for classification of labelled data into distinct sets, and are capable of modelling both linear and non-linear separable datasets via kernelization and projection into higher dimensional spaces. Recently, the Robust SVM (rSVM) has been proposed, which provides immunity to uncertainty in the dataset and has an easy reformulation as a second-order cone program (SOCP), a convex optimisation problem that is easily solvable by, e.g., interior-point methods. Other SVMs are the $L_1$-SVM, and the Doubly-regularized SVM (DrSVM). 

The aim of this report is to assess some of these SVMs, and some other classification techniques (including a decision tree, a naive Bayes approach, and a neural network) on a well-used dataset, the Wisconsin Breast Cancer dataset (available here: \url{https://www.kaggle.com/uciml/breast-cancer-wisconsin-data}). In particular, we wish to evaluate the rSVM in the case where the given training set is noisy/uncertain, and classifications need to be made taking this into account. 

This report is broken up into four sections:
\begin{enumerate}
	\item We describe the dataset and the preprocessing carried out on it, to prepare it for use. 
	\item We compare a Decision Tree, a Gaussian Naive Bayes classifier, a Two-Layered Perceptron (Neural Network), and a standard SVM by their performance on the (preprocessed) dataset. 
	\item We define the mathematical formulations of the classic SVM, the DrSVM, and the rSVM, that are used to implement the methods in MATLAB. MATLAB has been chosen for its more powerful optimisation toolkits and its more user-friendly interface. 
	\item Finally, we evaluate these three SVMs on the dataset, now affected by noise. 
\end{enumerate}

The selected metric over which we evaluate models is simply the raw accuracy of the model on the dataset, as expected on the Kaggle webpage.

\section{The Wisconsin Breast Cancer Dataset, and Preprocessing}

The dataset contains 569 items and 32 columns:
\begin{itemize}
	\item The first column is the item ID, numbered from 0 through 568. 
	\item The second column is the target, which is the diagnosis of the cancer as malignant (M) or benign (B). There are 357 benign diagnoses, and 212 malignant diagnoses, giving a class distribution of $[0.63, 0.37]$. 
	\item The remaining thirty columns contain the mean, standard error, and extremal value (worst-case measurement) of 10 features: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension. 
\end{itemize}
For this dataset the following preprocessing was done:
\begin{itemize}
	\item The ID column was dropped, as it does not affect the diagnosis. 
	\item The diagnoses were mapped to the numeric values: 0 if benign, 1 if malignant. 
	\item It was revealed there were no missing or NaN values in the dataset. 
	\item Each column was normalised, by recentering at 0 (subtracting the mean), and dividing through byb the standard deviation. 
	\item Finally, columns that had a correlation coefficient $< 0.1$ with the target value were dropped, as these were decided to be uncorrelated with the target, and therefore would not affect the diagnosis. 
\end{itemize}
The correlation of each feature with the diagnosis (ordered as in the dataset, so diagnosis at position 0, etc.) is shown below. 

\begin{figure}[htb!]
	\centering
	\includegraphics[width=0.75\textwidth]{corr}
\end{figure}

This process is carried out by \texttt{preprocessing.py}, which then saves the resulting dataset in a new csv file. The corresponding notebook also demonstrates this process step by step. 

\section{Comparison of Non-Robust Methods}

Taking our preprocessed dataset, we wish to make some preliminary comparisons of some common methods, to motivate the rest of the report. We chose the following (non-robust) methods:
\begin{itemize}
	\item A decision tree, with the minimum number of samples at each leaf set to 2\% of the training set, to avoid overfitting. It was determined that anything less than 1\% would overfit, while anything over 4\% would underfit. 
	\item A Gaussian Naive Bayes classifier. A Gaussian model was chosen since our data was previously normalised. To accommodate the Bayesian assumption of independence of features (which clearly doesn't hold between, for example, radius mean, standard error, and extremal value), this classifier only considered the extremal values of each core feature. 
	\item A Two-Layered Perceptron (neural network). The hidden layer was chosen to be twice the size of the input dimension (i.e. twice the number of features), with tanh activation at the hidden layer, and sigmoid activation at the output. The model was trained via stochastic gradient descent, and an adaptive learning rate initialised at 0.1. 
	\item A Support Vector Machine, with a linear kernel function and regularisation parameter set to 0.05. The regularisation parameter was determined through a grid search which showed the optimal choice to mostly lie in the range $[0, 0.1]$. 
\end{itemize}
For 100 simulations, a model was created and then trained on 80\% of the dataset, and tested on the remaining 20\%. For each simulation the accuracy (proportion of predictions which were correct) were recorded, as was the average of all simulations; see Table 1. We also recorded the F1 score for each model, for each simulation, to assess the reliability of the accuracy given the slight class imbalance; see Table 2. 

\begin{table}[H]
	\centering
	\captionsetup{font=footnotesize}
	\begin{tabular}{| c || c | c | c | c | c || c |}
		\hline
		\textbf{Model} & \textbf{Sim.	1} & \textbf{Sim. 2} & \textbf{Sim. 3} & \textbf{Sim. 4} & \textbf{Sim. 5} & \textbf{Average} 
		\tline\hline
		Decision Tree & 0.9211 & 0.9386 & 0.9474 & 0.9298 & 0.9561  & 0.9326 
		\tline\hline
		Gaussian NB & 0.9737 & 0.9474 & 0.9298 & 0.9474 & 0.9474 & 0.9445
		\tline\hline
		Neural Network & 0.9912 & 0.9825 & 1.0000 & 0.9386 & 0.9474 & 0.9729
		\tline\hline
		Support Vector Machine & 0.9912 & 0.9825 & 0.9912 & 0.9649 & 0.9737 & 0.9736
		\tline\hline
	\end{tabular}
	\caption{Accuracy results for the four basic models. Sim $i$ is the accuracy for the $i^{\rm th}$ simulation; Average is the average accuracy over 100 simulations.}
\end{table}

\begin{table}[H]
	\centering
	\captionsetup{font=footnotesize}
	\begin{tabular}{| c || c | c | c | c | c || c |}
		\hline
		\textbf{Model} & \textbf{Sim.	1} & \textbf{Sim. 2} & \textbf{Sim. 3} & \textbf{Sim. 4} & \textbf{Sim. 5} & \textbf{Average} 
		\tline\hline
		Decision Tree & 0.9072 & 0.9213 & 0.9211 & 0.8974 & 0.9398 & 0.9082
		\tline\hline
		Gaussian NB & 0.9684 & 0.9348 & 0.9070 & 0.9231 & 0.9250 & 0.9252
		\tline\hline
		Neural Network & 0.9897 & 0.9738 & 1.0000 & 0.9091 & 0.9231 & 0.9633
		\tline\hline
		Support Vector Machine & 0.9897 & 0.9783 & 0.9877 & 0.9487 & 0.9620 & 0.9638
		\tline\hline
	\end{tabular}
	\caption{F1 scores for the four basic models. Sim $i$ is the F1 score for the $i^{\rm th}$ simulation; Average is the average F1 score over 100 simulations.}
\end{table}
From the results we can see that, on average, the tuned SVM just outperforms the tuned Neural Network with one hidden layer, and is the best performing model of the four. We can also see the accuracies are reliable in that their deviation from the F1 score is small. Therefore, it is worth considering SVMs more generally in the rest of the report; in the following sections we will define four different SVM models and their mathematical formulations, and then compare their performance in a noisy setting. 

\section{Different SVM Models}

We now present the four different SVM models that we will compare in the next section: the standard SVM, the DrSVM, the HHSVM, and the robust SVM. Each of these have been implemented from scratch in MATLAB, with the use of the optimisation package MOSEK, interfaced through YALMIP. The corresponding MATLAB live script gives a simple example demonstrating the implementation and solution retrieval for each.

We denote the number of datapoints as $m$, and the number of features as $n$. 

\subsection{Standard SVM}

For this report, we take the standard SVM model as described in the lectures to be that with soft margins, as we do not want to assume linear separability. The primal formulation for this problem is
\begin{eqnarray*}
& \ds \min_{\pmb{w}\in\mathbb{R}^n,\gamma,\pmb{\xi}\in\mathbb{R}^m} & \frac{1}{2}\Vert \bm{w} \Vert_2^2  + \lambda\bm{e}^T\bm{\xi} 
\\
& \mbox{ subject to } & Y(X\bm{w} - \bm{e}\gamma) + \bm{\xi} \geq \bm{e} 
\\
& & \bm{\xi} \geq 0
\end{eqnarray*}
where $\bm{x}_i\in\mathbb{R}^n$ is the $i^{\rm th}$ data point, $X\in\mathbb{R}^{m\times n}$ is a matrix whose $i^{\rm th}$ row is $\bm{x}_i$, $\bm{y}\in\mathbb{R}^m$ is the classification vector (with each element $y_i$ in $\{-1, 1\}$), $Y = {\rm diag}(\bm{y})\in\mathbb{R}^{m\times m}$, and $\bm{e} = (1, 1, \dots, 1)^T\in\mathbb{R}^{m}$. The optimisation variables are $\bm{w}$, the weight vector, $\gamma$ the bias, and $\bm{\xi}$, the misclassification error for $X$: that is, $\xi_i = 0$ if ${\rm sign}(\bm{w}^T\bm{x}_i - \gamma) = y_i$, otherwise $\xi_i > 0$, $i=1,\dots,m$. 

Typically we solve $(SVM)$ by instead formulating and solving the dual problem, as this (a) allows for us to more efficiently the solve the problem if we wish to apply the kernel trick for high-dimensional embedding, and (b) still be able to retrieve the values for $\bm{w}$ and $\gamma$. The dual problem is given by
\begin{eqnarray*}
(SVM) & \ds\min_{\pmb{u}\in\mathbb{R}^m} & \frac{1}{2} \bm{u}^TYXX^TY^T\bm{u} - \bm{e}^T\bm{u}
\\
& \mbox{subject to } & \bm{e}^TY^T\bm{u} = 0
\\
& & 0 \leq \bm{u} \leq \lambda\bm{e}
\end{eqnarray*}
The original solution is then retrieved as follows: $\bm{x}_j$ is a support vector iff $0 < u_j < \lambda$, $j=1,\dots,m$; $\bm{w} = X^TY^T\bm{u}$; and $\gamma$ satisfies $y_j(\bm{w}^T\bm{x}_j - \gamma) = 1$ for support vector $\bm{x}_j$. 

In our experiments, we implement the dual formulation. 

\subsection{$L_1$-SVM \cite{l1norm}}

The $L_1$-norm SVM is very similar to the standard SVM, except that it replaces the regularization term for $\bm{w}$ with that given by use of the $L_1$ norm instead. 

\begin{eqnarray*}
& \ds \min_{\pmb{w}\in\mathbb{R}^n,\gamma,\pmb{\xi}\in\mathbb{R}^m} & \Vert \bm{w} \Vert_1  + \lambda\bm{e}^T\bm{\xi} 
\\
& \mbox{ subject to } & Y(X\bm{w} - \bm{e}\gamma) + \bm{\xi} \geq \bm{e} 
\\
& & \bm{\xi} \geq 0
\end{eqnarray*}
Due to the fact that $\Vert \bm{w} \Vert$ is not differentiable at $\bm{w} = \bm{0}$, an alternative reformulation for the above comes from Mangasarian \cite{pq}, wherein we set $\bm{w} = \bm{p} - \bm{q}$, for $\bm{p}, \bm{q} \in \mathbb{R}^n_+$. This gives rise to the alternative formulation as a standard linear program:
\begin{eqnarray*}
(L_1SVM) & \ds\min_{\bm{p},\bm{q}\in\mathbb{R}^n, \gamma, \bm{\xi}\in\mathbb{R}^m} & \bm{e}^T_n(\bm{p} + \bm{q}) + \lambda\bm{e}^T_m\bm{\xi}
\\
& \mbox{subject to } & Y(X(\bm{p} - \bm{q}) - \bm{e}_m\gamma) + \bm{\xi} \geq \bm{e}_m 
\\
& & \bm{p}, \bm{q}, \bm{\xi} \geq 0
\end{eqnarray*}
One well documented advantage of the above method is that, for large values of $\lambda$, the problem is forced to restrict some weights to 0. In this sense, the problem is also ideal for feature selection: the task of choosing which features are necessary for classification. Feature selection will not be dealt with in this report however. 

\subsection{DrSVM \cite{drsvm}}

The Doubly-Regularised SVM (DrSVM) aims to make the best of both of the previous methods, by using \textbf{two} regularization terms in the objective function: one is the previous $L_2$-norm regularization (or \emph{ridge} regularization); the other is the $L_1$-norm (or \emph{lasso}) regularization. The primal formulation is thus given by
\begin{eqnarray*}
(DrSVM) & \ds\min_{\pmb{w}\in\mathbb{R}^n, \gamma, \pmb{\xi}\in\mathbb{R}^m} & \ds\frac{\lambda_1}{2}\Vert \bm{w} \Vert_2^2 + \lambda_1 \Vert \bm{w} \Vert_1 + \bm{e}^T\bm{\xi}
\\
& \mbox{subject to } & Y(X\bm{w} - \bm{e}\gamma) + \bm{\xi} \geq \bm{e} 
\\
& & \bm{\xi} \geq 0
\end{eqnarray*}

A recent method for efficiently solving the above problem was recently proposed in \cite{pqsvm} which, inspired by the approach by Mangasarian as above, also uses the substitution $\bm{w} = \bm{p} - \bm{q}$ and then, under some simple assumptions, proposes a dual that is quadratic program:
\begin{eqnarray*}
& \ds\min_{\bm{u}\in\mathbb{R}^m} & \frac{1}{2}\bm{u}^T\left( \hat{Y}\left(\hat{X}\left(C + \nu I_{2n+m}\right)^{-1}\hat{X}^T + \bm{e}\bm{e}^T\right)\hat{Y} + M\right)\bm{u} - \bm{e}^T\bm{u}
\end{eqnarray*}
where each matrix is in $\mathbb{R}^{(2n+m)\times(2n+m)}$ and is given by
\[
\hat{Y} = \begin{pmatrix} Y & 0 \\ 0 & 0 \end{pmatrix}, \ \hat{X} = \begin{pmatrix} 0 & X & -X \\ 0 & 0 & 0 \end{pmatrix}, \ C = \lambda_1\begin{pmatrix} 0 & 0 & 0 \\ 0 & I_n & 0 \\ 0 & 0 & I_n \end{pmatrix}, \mbox{ and } M = (C+\nu I_{2n+m})^{-1}(2\hat{Y}\hat{X}+I_{2n+m})
\]
To retrieve our original solution, we do the following:
\[
\begin{bmatrix} \bm{\xi} \\ \bm{p} \\ \bm{q} \end{bmatrix} = (C + \nu I_{2n+m})^{-1}((\hat{Y}\hat{X})^T + I_{2n+m})\bm{u}, \enspace, \gamma = -\bm{e}^T\hat{Y}\bm{u}, \bm{w} = \bm{p} - \bm{q}
\]

\subsection{Robust SVM \cite{rsvm}}

Finally we reach the robust SVM, the main model we wish to evaluate in this paper, in comparison to the non-robust approaches. In practice, the input data is sensitive to error, be it measurement error, data uncertainty, noise, etc. The robust approach makes no assumption on the nature of this uncertainty other than that, for some radius $r_i>0$, $i=1,\dots,m$, each data point is bounded in an $n$-dimensional ball:
\[
\bm{x}_i\in\mathcal{U}_i(r_i) = \bar{\bm{x}}_i + r_i\mathbb{B}_n, \quad i=1,\dots,m
\]
where $\mathbb{B}_n = \left\{\bm{v}\in\mathbb{R}^n : \Vert \bm{v}\Vert_2\leq 1\right\}$ is an $n$-dimensional ball. The robust methodology aims to take this uncertainty into account, and so find a solution to our standard SVM problem that is feasible (satisfies all constraints) no matter where the data points lie in their uncertainty sets $\mathcal{U}_i$:
\begin{eqnarray*}
& \ds\min_{\bm{w}\in\mathbb{R}^n, \gamma, \bm{\xi}\in\mathbb{R}^m} & \frac{1}{2}\Vert \bm{w} \Vert_2^2 + \lambda\bm{e}^T\bm{\xi}
\\
& \mbox{subject to } & y_i(\bm{x}_i^T\bm{w} - \gamma) + \xi_i \geq 1, \enspace \forall \bm{x}_i\in\mathcal{U}_i(r_i),\enspace i=1,\dots,m
\\
& & \xi_i \geq 0, \enspace i=1,\dots,m, 
\end{eqnarray*}
Following the robust methodology and the approach in \cite{rsvm}, we reformulate the above into a second-order cone program, that no longer has the semi-infinite constraints posed by the forall quantifier:
\begin{eqnarray*}
(rSVM) & \ds\min_{\bm{w}\in\mathbb{R}^n, \gamma, \bm{\xi}\in\mathbb{R}^m, t_1, t_2} & \frac{1}{2}t_1^2 + \lambda t_2
\\
& \mbox{subject to } & y_i\left(\bar{\bm{x}}_i^T\bm{w}-\gamma\right) - y_ir_i\Vert\bm{w}\Vert_2 + \xi_i \geq 1
\\
& & \Vert \bm{w} \Vert_2 \leq t_1
\\
& & \bm{e}^T\bm{\xi} \leq t_2
\\
& & \bm{\xi}\geq 0
\end{eqnarray*}
Notice that in the case where there is no uncertainty (i.e. $r_i=0$, $i=1,\dots,m$) then $(rSVM)$ reduces to $(SVM)$, as would be expected.

\subsection*{Simple Comparison}

A simple comparison between the four methods is demonstrated below. The plots were generated with \texttt{svms\_example.m}, and can also be examined interactively with the accompanying MATLAB Live script.

\begin{figure}[hbt!]
	\begin{subfigure}{.5\textwidth}
		\centering
		\captionsetup{font=footnotesize}
		\includegraphics[width=\linewidth]{stand}
		\caption{Standard SVM model. Support vectors in green.}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		\captionsetup{font=footnotesize}
		\includegraphics[width=\linewidth]{l1}
		\caption{$L_1$-SVM model.}
	\end{subfigure}
	\newline
	\begin{subfigure}{.5\textwidth}
		\centering
		\captionsetup{font=footnotesize}
		\includegraphics[width=\linewidth]{dr}
		\caption{DrSVM model.}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		\captionsetup{font=footnotesize}
		\includegraphics[width=\linewidth]{robust}
		\caption{Robust SVM model. Uncertainty sets circled. In this case, the ``support vector'' is the cross located at approximate $(3.125,3.25)$, as the margin borders the uncertainty set.}
	\end{subfigure}
\end{figure}

As we can see, the robust method does not identify any of the points as support vectors. This is because, inherent in the methodology, the formulation actually considers the \emph{worst-case} in the uncertainty set surrounding each point. To this effect, we can observe that the cross located at approximately $(3.125,3.25)$ is the ``support vector'' chosen, as the margin borders the uncertainty set at its extremal value (in the 2-norm sense). 

We can now progress to testing our four SVM models with regards to their resilience in the face of uncertainty. 

\section{Classification Under Noisy Data}
In order to evaluate the worth of the rSVM method in comparison to the SVM, the $L_1$SVM, and the DrSVM, we will perform the following experiment:
\begin{itemize}
	\item Firstly, after splitting our dataset into training and test sets, we will choose small values for $r_i$, $i=1,\dots,m$, and perturb our $m$ training data points within the uncertainty set $\mathcal{U}_i(r_i)$. 
	\item We will then train our four models on this perturbed training set. 
	\item The four models will then be tested on the \emph{original} test set, and evaluated on (a) their accuracy, and (b) their F1 score. 
\end{itemize}
The motivation behind this is as follows: given a noisy dataset, we will be testing how well our four models can classify true data. 

\begin{thebibliography}{99}

\bibitem{pqsvm} M. Dunbar, J. Murray, L. A. Cysique, B. J. Brew, V. Jeyakumar: ``Simultaneous classification and feature selection via convex quadratic programming with application to HIV-associated neurocognitive disorder assessment'', \emph{European Journal of Operational Research}, (206), 470-478, 2010. 

\bibitem{rsvm} M. A. Goberna, V. Jeyakumar, G. Li: ``Calculating Radius of Robust Feasibility of Uncertain Linear Conic Programs via Semidefinite Programs'', \emph{UNSW Preprint}, \url{https://arxiv.org/abs/2007.07599}, 2020.

\bibitem{pq} O. L. Mangasarian: ``Exact 1-Norm Support Vector Machines via Unconstrained Convex Differentiable Minimization'', \emph{Journal of Machine Learning Research}, (7), 1517-1530, 2006.

\bibitem{drsvm} L. Wang, J. Zhu, and H. Zou: ``The Doubly Regularized Support Vector Machine'', \emph{Statistica Sinica}, (16), 589â€“615, 2006.

\bibitem{l1norm} J. Zhu, S. Rosset, T. Hastie, R. Tibshirani: ``1-Norm support vector machines'', \url{http://www stat.stanford.edu/~hastie/Papers/}, 2003. 






\end{thebibliography}

\end{document}